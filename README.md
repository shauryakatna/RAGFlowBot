
<!-- TOC --><a name="cognito-langgraph-rag"></a>
# RAGFlowBot

- Developed an advanced RAG workflow utilizing LangGraph to enhance question-answering accuracy, resulting in a significant reduction in Large Language Model (LLM) hallucinations. 
- Architected a stateful, multi-step process incorporating document retrieval, relevance grading, and web search fallback using LangGraph, leading to improved information retrieval and user satisfaction. 
- Integrated Large Language Models (LLMs) with a graph database and a novel self-reflection workflow to efficiently store, retrieve, and validate information, enhancing the chatbot's ability to provide accurate and relevant answers to complex user queries. 
- Implemented a self-reflection mechanism within the chatbot to iteratively refine responses, ensuring that generated answers are grounded in provided documentation, thereby minimizing hallucinations and increasing user trust. 
- Implemented full-stack solutions utilizing ChromaDB for efficient vector storage and retrieval, ensuring seamless integration with LangChain and secure handling of environment configurations through dotenv, which led to a 30% reduction in latency during information retrieval.
- Architected and fine-tuned a comprehensive LLM technology stack that includes LangChain, LangGraph, and external web search APIs (Tavily Search), achieving a resilient AI system capable of mitigating the challenges of LLM hallucinations through strategic reinforcement learning and retrieval augmentation.
- Spearheaded the development of an advanced chatbot utilizing Retrieval-Augmented Generation (RAG) workflows to enhance question-answering accuracy and mitigate LLM hallucinations.
- Leveraged LangGraph to construct a stateful, multi-step process encompassing document retrieval, relevance grading, and web search fallback, resulting in a more robust and accurate chatbot.
- Agentic AI: Designed autonomous agents capable of dynamic decision-making within the chatbot, enhancing its adaptability to diverse user queries.

## Purpose of Project

This project implements an advanced Retrieval Augmented Generation (RAG) workflow chatbot to enhance question-answering accuracy and reduce LLM hallucinations. It leverages LangGraph to create a stateful, multi-step process that includes document retrieval, relevance grading, and web search fallback. This project aims to create a documentation bot that can answer user questions based on provided documentation. It leverages Large Language Models (LLMs), a graph database, and a novel self-reflection workflow to efficiently store, retrieve, and validate information.

Abstract: The primary goal of this project is to streamline the process of accessing and understanding documentation. By utilizing LLMs, a graph database, and a self-reflection workflow, the bot can provide accurate and relevant answers to user queries, even if they are complex or phrased differently from the original documentation. The bot also aims to minimize hallucinations and ensure that the generated answers are grounded in the provided documentation.


## Input and Output

**Input:** Natural language questions from users.


- a list of URLs

```python
urls = [
    "https://lilianweng.github.io/posts/2024-11-28-reward-hacking/",
    "https://lilianweng.github.io/posts/2024-07-07-hallucination/",
    "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/",
    "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/"
]
```

- Question 1:

```python
what is agent memory?
```


**Output:** Coherent and contextually relevant responses generated by the LLM, augmented with retrieved documents and/or web search results.

<img src="https://github.com/user-attachments/assets/c86f3e10-e518-484b-95d4-49687b0cf5cb" width="40%" height="40%">

```python
Hello Advanced RAG!
%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
 __start__([<p>__start__</p>]):::first
 retrieve(retrieve)
 grade_documents(grade_documents)
 generate(generate)
 websearch(websearch)
 __end__([<p>__end__</p>]):::last
 retrieve --> grade_documents;
 websearch --> generate;
 __start__ -.-> websearch;
 __start__ -.-> retrieve;
 grade_documents -.-> websearch;
 grade_documents -.-> generate;
 generate -. &nbsp;useful&nbsp; .-> __end__;
 generate -. &nbsp;not useful&nbsp; .-> websearch;
 generate -. &nbsp;not supported&nbsp; .-> generate;
 classDef default fill:#f2f0ff,line-height:1.2
 classDef first fill-opacity:0
 classDef last fill:#bfb6fc

---route question---
---route question to RAG---
---Retrieving---
---check document relevance to question---
---grade: document not relevant---
---grade: document not relevant---
---grade: document not relevant---
---grade: document not relevant---
---assess granted documents---
---decision: not all docs are relevant to question ---
---web search---
State keys: dict_keys(['question', 'web_search', 'documents'])
---generate---
---check hallucination---
---decision: generation is grounded in documents---
---grade generation vs question---
---decision: generation addresses question---
{'question': 'what is agent memory?', 'generation': 'Agent memory is a crucial component in artificial intelligence that allows AI models to retain information across interactions rather than treating every query as a new conversation. It helps the agent to better plan and is used to store and recall information. Agents are stateful and they use this memory to store previous interactions and use them in subsequent calls.', 'web_search': True, 'documents': [Document(metadata={}, page_content='In general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan\nUnderstanding Agent Memory in AI: Types, Use Cases, and Implementation | by Siladitya Ghosh | Feb, 2025 | Medium Understanding Agent Memory in AI: Types, Use Cases, and Implementation The secret lies in agent memory — a crucial component that allows AI to store and recall information across interactions. What agent memory is and why it’s essential The different types of memory used in AI agents Real-world use cases of memory in AI systems By the end, you’ll have a solid understanding of how AI agents use memory and how to implement it in your own projects. What is Agent Memory? Agent memory allows AI models to retain information across interactions rather than treating every query as a new conversation. Types of Agent Memory\nTowards AGI: [Part 1] Agents with Memory - SuperAGI Towards AGI: [Part 1] Agents with Memory Agents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. However, agents store the previous interactions in variables and use it in subsequent LLM calls. Therefore, agents are stateful and they have memory. Deep dive into various types of Agent Memory LTM type 2: Semantic memory (aka Reflections): It stores an agent’s knowledge about the world and itself. LTM type 3: Procedural memory: This memory represents the agent’s procedures for thinking, acting, decision-making, etc. AI Agent Systems for fully automated sales, marketing, support and app development\nEnroll now: https://bit.ly/3YwWJeR Build agentic memory into your applications with LLMs as Operating Systems: Agent Memory, a short course\nMemory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?')]}

Process finished with exit code 0

```

![image](https://github.com/user-attachments/assets/fe81a1eb-fc84-41db-98e0-e203de36d3c0)

LangSmith trace: https://smith.langchain.com/public/436a9f0e-f390-468a-bb0b-eaa9df0e08ce/r

![image](https://github.com/user-attachments/assets/34e890f2-6092-4ed7-aea7-dd2ff57a7e33)

Output:

- Web Search: `true`
- Generation: `Agent memory is a crucial component in artificial intelligence that allows AI models to retain information across interactions rather than treating every query as a new conversation. It helps the agent to better plan and is used to store and recall information. Agents are stateful and they use this memory to store previous interactions and use them in subsequent calls.`
- Documents: `In general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan
Understanding Agent Memory in AI: Types, Use Cases, and Implementation | by Siladitya Ghosh | Feb, 2025 | Medium Understanding Agent Memory in AI: Types, Use Cases, and Implementation The secret lies in agent memory — a crucial component that allows AI to store and recall information across interactions. What agent memory is and why it’s essential The different types of memory used in AI agents Real-world use cases of memory in AI systems By the end, you’ll have a solid understanding of how AI agents use memory and how to implement it in your own projects. What is Agent Memory? Agent memory allows AI models to retain information across interactions rather than treating every query as a new conversation. Types of Agent Memory
Towards AGI: [Part 1] Agents with Memory - SuperAGI Towards AGI: [Part 1] Agents with Memory Agents are an emerging class of artificial intelligence (AI) systems that use large language models (LLMs) to interact with the world. However, agents store the previous interactions in variables and use it in subsequent LLM calls. Therefore, agents are stateful and they have memory. Deep dive into various types of Agent Memory LTM type 2: Semantic memory (aka Reflections): It stores an agent’s knowledge about the world and itself. LTM type 3: Procedural memory: This memory represents the agent’s procedures for thinking, acting, decision-making, etc. AI Agent Systems for fully automated sales, marketing, support and app development
Enroll now: https://bit.ly/3YwWJeR Build agentic memory into your applications with LLMs as Operating Systems: Agent Memory, a short course
Memory for agents Memory for agents At Sequoia’s AI Ascent conference in March, I talked about three limitations for agents: planning, UX, and memory. But what even is memory? While the exact shape of memory that your agent has may differ by application, we do see different high level types of memory. Below is my rough, ELI5 explanation of each type and practical ways for how todays agents may use and update this memory type. Besides just thinking about the type of memory to update in their agents, we also see developers thinking about how to update agent memory. One way to update agent memory is “in the hot path”. Why do we care about memory for agents?`


- Question 2:

```python
what is GRPO and MoE in DeepSeek architecture?
```

Output:

```python
Hello Advanced RAG!
%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
 __start__([<p>__start__</p>]):::first
 retrieve(retrieve)
 grade_documents(grade_documents)
 generate(generate)
 websearch(websearch)
 __end__([<p>__end__</p>]):::last
 retrieve --> grade_documents;
 websearch --> generate;
 __start__ -.-> websearch;
 __start__ -.-> retrieve;
 grade_documents -.-> websearch;
 grade_documents -.-> generate;
 generate -. &nbsp;useful&nbsp; .-> __end__;
 generate -. &nbsp;not useful&nbsp; .-> websearch;
 generate -. &nbsp;not supported&nbsp; .-> generate;
 classDef default fill:#f2f0ff,line-height:1.2
 classDef first fill-opacity:0
 classDef last fill:#bfb6fc

---route question---
---route question to websearch---
---web search---
State keys: dict_keys(['question'])
---generate---
---check hallucination---
---decision: generation is grounded in documents---
---grade generation vs question---
---decision: generation addresses question---
{'question': 'what is GRPO and MoE in DeepSeek architecture?', 'generation': 'Group Relative Policy Optimization (GRPO) is a reinforcement learning technique developed by DeepSeek to enhance the reasoning capabilities of large language models. It regularizes by directly adding the KL divergence between the trained policy and the reference policy to the objective function. MoE (Mixture of Experts) is not responsible for emergent reasoning in DeepSeek-R1 architecture, as evidenced by the DeepSeek-R1-Distill-Qwen-32B model, which maintains all of DeepSeek-R1’s reasoning capabilities despite being fully dense and not having MoE.', 'documents': [Document(metadata={}, page_content='“We intentionally limit our constraints to this structural format, avoiding any content-specific biases — such as mandating reflective reasoning or promoting particular problem-solving strategies — to ensure that we can accurately observe the model’s natural progression during the RL process.” — DeepSeek-R1-Zero/R1 technical report Group Relative Policy Optimization (GRPO) is a reinforcement learning (RL) technique developed by DeepSeek to enhance the reasoning capabilities of large language models (LLMs). “Instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the objective function, avoiding complicating the calculation of 𝐴_𝑖.” — DeepSeekMath “The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question” — DeepSeekMath\nThe GRPO algorithm is at the heart of the newest DeepSeek R1 architecture. In this tutorial, we will discuss the details of the formula\nDeepSeek-R1-Distill-Qwen-32B is not an MoE model and yet it retains all reasoning properties of DeepSeek-R1. If MoE were essential to DeepSeek-R1’s emergent reasoning, then distilling the model into a fully dense architecture should have eliminated its ability to perform deep, structured reasoning. Since DeepSeek-R1-Distill-Qwen-32B has no MoE and retains its reasoning abilities, this is direct empirical proof that MoE is not responsible for emergent reasoning. The strongest empirical refutation of the claim that DeepSeek-R1’s reasoning abilities require MoE lies in the very nature of DeepSeek-R1-Distill-Qwen-32B, a model that maintains all of DeepSeek-R1’s reasoning capabilities despite being fully dense. It is a smoking gun proof that DeepSeek-R1’s emergent reasoning behaviors are model agnostic and stem purely from reinforcement learning, structured optimization, and Chain-of-Thought training, not from architectural sparsity or expert gating.\nExplore the groundbreaking advancements of DeepSeek R1, a reinforcement learning-driven reasoning model. Unlike traditional models that heavily rely on supervised fine-tuning, DeepSeek R1 adopts a reinforcement learning-only (RL-only) approach to develop its reasoning capabilities. DeepSeek-R1 was trained using GRPO instead of PPO, as it allowed reinforcement learning on a large-scale language model without requiring a critic network. By learning from DeepSeek V3’s evolution, DeepSeek R1 was able to push the boundaries of reinforcement learning-driven reasoning, achieving state-of-the-art performance in complex problem-solving tasks while maintaining computational efficiency. DeepSeek R1 represents a paradigm shift in reinforcement learning-driven reasoning models, proving that logical self-improvement can emerge without supervised fine-tuning.\nThey used this data to train DeepSeek-V3-Base on a set of high quality thoughts, they then pass the model through another round of reinforcement learning, which was similar to that which created DeepSeek-r1-zero, but with more data (we’ll get into the specifics of the entire training pipeline later). They took DeepSeek-V3-Base, with these special tokens, and used GRPO style reinforcement learning to train the model on programming tasks, math tasks, science tasks, and other tasks where it’s relatively easy to know if an answer is correct or incorrect, but requires some level of reasoning.')]}

Process finished with exit code 0
```


![image](https://github.com/user-attachments/assets/3b008240-1cd4-4e3d-8f4a-d4551b05b675)

LangSmith trace: https://smith.langchain.com/public/46a28e4e-c7ed-494b-8cfd-14ea7b6d3fe6/r

![image](https://github.com/user-attachments/assets/13cb7386-f797-4dec-96ca-1eeab1f958ca)

Output:

- Generation: `Group Relative Policy Optimization (GRPO) is a reinforcement learning technique developed by DeepSeek to enhance the reasoning capabilities of large language models. It regularizes by directly adding the KL divergence between the trained policy and the reference policy to the objective function. MoE (Mixture of Experts) is not responsible for emergent reasoning in DeepSeek-R1 architecture, as evidenced by the DeepSeek-R1-Distill-Qwen-32B model, which maintains all of DeepSeek-R1’s reasoning capabilities despite being fully dense and not having MoE.`
- Documents: `“We intentionally limit our constraints to this structural format, avoiding any content-specific biases — such as mandating reflective reasoning or promoting particular problem-solving strategies — to ensure that we can accurately observe the model’s natural progression during the RL process.” — DeepSeek-R1-Zero/R1 technical report Group Relative Policy Optimization (GRPO) is a reinforcement learning (RL) technique developed by DeepSeek to enhance the reasoning capabilities of large language models (LLMs). “Instead of adding KL penalty in the reward, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the objective function, avoiding complicating the calculation of 𝐴_𝑖.” — DeepSeekMath “The group relative way that GRPO leverages to calculate the advantages, aligns well with the comparative nature of rewards models, as reward models are typically trained on datasets of comparisons between outputs on the same question” — DeepSeekMath
The GRPO algorithm is at the heart of the newest DeepSeek R1 architecture. In this tutorial, we will discuss the details of the formula
DeepSeek-R1-Distill-Qwen-32B is not an MoE model and yet it retains all reasoning properties of DeepSeek-R1. If MoE were essential to DeepSeek-R1’s emergent reasoning, then distilling the model into a fully dense architecture should have eliminated its ability to perform deep, structured reasoning. Since DeepSeek-R1-Distill-Qwen-32B has no MoE and retains its reasoning abilities, this is direct empirical proof that MoE is not responsible for emergent reasoning. The strongest empirical refutation of the claim that DeepSeek-R1’s reasoning abilities require MoE lies in the very nature of DeepSeek-R1-Distill-Qwen-32B, a model that maintains all of DeepSeek-R1’s reasoning capabilities despite being fully dense. It is a smoking gun proof that DeepSeek-R1’s emergent reasoning behaviors are model agnostic and stem purely from reinforcement learning, structured optimization, and Chain-of-Thought training, not from architectural sparsity or expert gating.
Explore the groundbreaking advancements of DeepSeek R1, a reinforcement learning-driven reasoning model. Unlike traditional models that heavily rely on supervised fine-tuning, DeepSeek R1 adopts a reinforcement learning-only (RL-only) approach to develop its reasoning capabilities. DeepSeek-R1 was trained using GRPO instead of PPO, as it allowed reinforcement learning on a large-scale language model without requiring a critic network. By learning from DeepSeek V3’s evolution, DeepSeek R1 was able to push the boundaries of reinforcement learning-driven reasoning, achieving state-of-the-art performance in complex problem-solving tasks while maintaining computational efficiency. DeepSeek R1 represents a paradigm shift in reinforcement learning-driven reasoning models, proving that logical self-improvement can emerge without supervised fine-tuning.
They used this data to train DeepSeek-V3-Base on a set of high quality thoughts, they then pass the model through another round of reinforcement learning, which was similar to that which created DeepSeek-r1-zero, but with more data (we’ll get into the specifics of the entire training pipeline later). They took DeepSeek-V3-Base, with these special tokens, and used GRPO style reinforcement learning to train the model on programming tasks, math tasks, science tasks, and other tasks where it’s relatively easy to know if an answer is correct or incorrect, but requires some level of reasoning.`


## Challenges and Difficulties

- Accuracy: Ensuring the bot provides accurate and relevant answers, especially for complex or ambiguous questions.
- Hallucination: Preventing the LLM from generating answers that are not supported by the provided documentation.
- Data Preprocessing: Efficiently processing and structuring documentation data for storage and retrieval.
- Scalability: Scaling the system to handle large volumes of documentation and user queries.
- Explainability: Providing clear explanations of how the bot arrived at its answers.
- Latency: Minimizing response time to provide a seamless user experience.


## Setup

Prerequisites
- Python 3.10+
-  OpenAI API key
-  Tavily API key
- Install required packages: `pip install -r requirements.txt`

1.  Clone the repository.
2.  Install the required packages: `pip install -r requirements.txt`
3.  Create a `.env` file and add your OpenAI and Tavily API keys:

```
OPENAI_API_KEY=
LANGCHAIN_API_KEY=
LANGCHAIN_PROJECT=
LANGCHAIN_TRACING_V2=true
TAVILY_API_KEY=
PYTHONPATH=/Users/junfanzhu/Desktop/langgraph
```

4.  Run the main application: `python graph/graph.py`



## Sumary 
This project offers a novel approach to documentation access and retrieval by incorporating a self-reflection workflow. By combining LLMs with a graph database and adaptive routing, it provides an efficient and user-friendly way to find accurate and relevant answers to questions. While there are challenges to overcome, the potential benefits are significant.The core of this project is to build a robust and reliable question-answering system using Retrieval Augmented Generation, but with a focus on mitigating common LLM challenges like hallucinations and irrelevant responses. We've implemented an advanced RAG pipeline that goes beyond basic retrieval. 

First, we leverage LangGraph to orchestrate a stateful workflow, which allows us to manage complex interactions between different components. This is crucial because we're not just retrieving and generating; we're also grading the relevance of retrieved documents. The key innovation here is the 'grade_documents' node. We use an LLM to assess the semantic relevance of each retrieved document to the user's query. If a document is deemed irrelevant, we trigger a web search using Tavily Search as a fallback, ensuring that the final response is comprehensive and accurate. This conditional logic, managed by LangGraph, is what distinguishes this system. We've also focused on modularity. Each step—retrieval, grading, web search, and generation—is encapsulated in its own node, making the system highly extensible. We're using ChromaDB for efficient vector storage and retrieval, and OpenAI's LLMs for both embedding and generation. 

In essence, this project tackles the 'garbage in, garbage out' problem by actively filtering and augmenting the retrieved context. The LangGraph framework allows us to create a pipeline that dynamically adapts to the quality of retrieved information, resulting in more reliable and accurate responses compared to a basic RAG setup. This design shows an understanding of how to build complex LLM applications that address real-world challenges."








